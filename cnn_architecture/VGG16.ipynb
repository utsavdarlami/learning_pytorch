{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16\n",
    "### From Paper\n",
    "- https://arxiv.org/pdf/1409.1556.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points from Paper\n",
    "- Input is a fixed-size 224×224 RGB image.\n",
    "- The only pre-processing we do is subtracting the mean RGB value, computedon the training set, from each pixel.\n",
    "- The image is passed through a stack of convolutional (conv.)layers, where we use filters with a very small receptive field:3×3. In one of the configurations we also utilise 1×1 convolution filters, which can be seen asa linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3×3 conv layers.\n",
    "\n",
    "- Max-pooling is performed over a 2×2pixel window, with stride 2\n",
    "- The first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class).\n",
    "-  Namely, the training is carriedout by optimising the multinomial logistic regression objective using mini-batch gradient descent(based on back-propagation (LeCun et al., 1989)) with momentum.  The batch size was set to256,momentum to0.9.  The training was regularised by weight decay (theL2penalty multiplier set to5·10−4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to0.5).The learning rate was initially set to10−2, and then decreased by a factor of10when the validationset accuracy stopped improving. In total, the learning ratewas decreased 3 times, and the learningwas stopped after370K iterations (74 epochs). \n",
    "- Dataset : ILSVRC-2012 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./VGG_Archi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note** : *We will implement configuration D*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{available_device} is available\")\n",
    "device = torch.device(device=available_device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
