* Graph Attention Network

- References :
  - https://nn.labml.ai/graphs/gat/index.html
  - https://www.youtube.com/watch?v=A-yKQamf2Fc [Understanding Graph Attention Networks]
  - https://www.youtube.com/watch?v=CwsPoa7z2c8 [Pytorch Geometric tutorial: Graph attention networks (GAT) implementation]
  - https://github.com/pyg-team/pytorch_geometric/issues/2029
  - https://arxiv.org/abs/1710.10903
- Questions :
--- 

#+DOWNLOADED: screenshot @ 2022-03-02 22:44:27
#+CAPTION:  
#+attr_html: :width 700 :height 400 :target /blogs
[[file:Graph_Attention_Network/2022-03-02_22-44-27_screenshot.png]]

#+DOWNLOADED: screenshot @ 2022-03-02 22:47:13
#+CAPTION:  

[[file:Graph_Attention_Network/2022-03-02_22-47-13_screenshot.png]]

** How to reshape the embeds to pass the attention $a$

#+DOWNLOADED: screenshot @ 2022-03-02 23:05:53
#+CAPTION:  
#+attr_html: :width 700 :height 400 :target /blogs
[[file:Graph_Attention_Network/2022-03-02_23-05-53_screenshot.png]]

#+begin_src python
out_embed_size = 4
N = 2
h = # [N, 4]
a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_embed_size)
#+end_src
** From Pytorch geometric

#+DOWNLOADED: screenshot @ 2022-03-02 23:24:48
[[file:Graph_Attention_Network/2022-03-02_23-24-48_screenshot.png]]

** My Implementation

#+begin_src python
class DummyGATLayer(gnn.MessagePassing):
    def __init__(self, in_channels=1, out_channels=2):
        super().__init__(aggr='add')
        
        self.linear1 = tnn.Linear(in_features=in_channels, out_features=out_channels)
        self.attn_linear = tnn.Linear(in_features=2*out_channels, out_features=1)
        
        self.leaky_relu = tnn.LeakyReLU(negative_slope=0.2)
        
    def forward(self, x , edge_index, edge_attrs=None):
        
        edge_index, _  = add_self_loops(edge_index, num_nodes=x.shape[0])
        h = self.linear1(x)
        out = self.propagate(edge_index, x=x, h=h)
        return out
    
    def message(self, h_i, h_j, edge_index, index):
                
        # Here we Concat the src node embeds and dst node embeds/features
        cat = torch.cat([h_i, h_j], dim=1) # [num_nodes, 2 * out_channels]
               
        # now need to pass this concat feature to the feed-forward layer to get attention score
        e_j = self.attn_linear(cat) # [num_nodes, 1]
        alphas = softmax(self.leaky_relu(e_j), index=index) # performs softmax with the neighboring nodes.       
            
        msg = alphas * h_j

        return msg
#+end_src
#+DOWNLOADED: screenshot @ 2022-03-02 22:44:27
#+CAPTION:  
#+attr_html: :width 700 :height 400 :target /blogs
[[file:Graph_Attention_Network/2022-03-02_22-44-27_screenshot.png]]

#+DOWNLOADED: screenshot @ 2022-03-02 22:47:13
#+CAPTION:  

[[file:Graph_Attention_Network/2022-03-02_22-47-13_screenshot.png]]
