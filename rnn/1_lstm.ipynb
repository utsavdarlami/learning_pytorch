{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Tutorial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torchtext.__version__\n",
    "# torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating deterministic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42  # The answer to life, the universe, and everything\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "available_device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(available_device)\n",
    "print(device)\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -f movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "imdb_df = pd.read_csv(\"movie_data.csv\")\n",
    "print(imdb_df.shape)\n",
    "del imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'fox', 'ca', \"n't\", 'jump', 'over', 'a', 'lazy', 'dog', '.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "tokenizer(\"The quick fox can't jump over a lazy dog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', # tokenizer \n",
    "                  tokenizer_language='en_core_web_sm', #none\n",
    "                 ) \n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "fields = [('REVIEWS', TEXT), ('LABEL', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data.TabularDataset(path='movie_data.csv', \n",
    "                              format='csv',\n",
    "                              fields=fields,\n",
    "                              skip_header=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = dataset.split(split_ratio=[0.9, 0.1],\n",
    "                                    random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = train_set.split(split_ratio=[0.9, 0.1],\n",
    "                                    random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 20002\n",
      "Label size : 2\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "TEXT.build_vocab(train_set, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_set)\n",
    "\n",
    "print(f\"Vocab size : {len(TEXT.vocab)}\")\n",
    "print(f\"Label size : {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we only build the vocabulary on the training set?**\n",
    "- When testing any machine learning system you do not want to look at the test set in any way.\n",
    "  We do not include the validation set as we want it to reflect the test set as much as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 464964),\n",
       " (',', 441024),\n",
       " ('.', 378556),\n",
       " ('a', 250866),\n",
       " ('and', 250751),\n",
       " ('of', 231374),\n",
       " ('to', 214406),\n",
       " ('is', 173465),\n",
       " ('in', 141132),\n",
       " ('I', 125873)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_commom_words = TEXT.vocab.freqs.most_common(10)\n",
    "top_commom_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also check the labels, ensuring 0 is for O(-ve) labeled review and 1 is for 1(+ve) review .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'0': 0, '1': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1': 20235, '0': 20265})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the data is creating the iterators.\n",
    "We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
    "\n",
    "We'll use a BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
    "\n",
    "We also want to place the tensors returned by the iterator on the GPU (if you're using one). \n",
    "PyTorch handles this using torch.device, we then pass this device to the iterator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# https://torchtext.readthedocs.io/en/latest/data.html#bucketiterator\n",
    "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                                              (train_set, val_set, test_set),\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              sort_within_batch=False,          #\n",
    "                                              sort_key=lambda x: len(x.REVIEWS), # \n",
    "                                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1013, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([52, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for t_i in train_iterator:\n",
    "    print(t_i.REVIEWS.size())\n",
    "    print(t_i.LABEL.size())\n",
    "    break\n",
    "\n",
    "for v_i in val_iterator:\n",
    "    print(v_i.REVIEWS.size())\n",
    "    print(v_i.LABEL.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalayzer(nn.Module):\n",
    "    def __init__(self,input_dim=20002, embedding_size=128, hidden_dim=256, num_classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [sent len, batch size]\n",
    "        \n",
    "        embeddings = self.embedding(x)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embeddings)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "        \n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = SentimentAnalayzer(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.005\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Next, we'll define our loss function. In PyTorch this is commonly called a criterion.\n",
    "\n",
    "The loss function here is binary cross entropy with logits.\n",
    "\n",
    "Our model currently outputs an unbound real number. As our labels are either 0 or 1, \n",
    "we want to restrict the predictions to a number between 0 and 1. We do this using the sigmoid or logit functions.\n",
    "\"\"\"\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# The BCEWithLogitsLoss criterion carries out both the sigmoid and the binary cross entropy steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy measuring function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(preds, y):\n",
    "    # preds = [batch_size]\n",
    "    # y = [batch_size, 1]\n",
    "    \n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc  = correct.sum() / len(correct)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            out = model(batch.REVIEWS)\n",
    "            scores = out.squeeze(1)\n",
    "            \n",
    "            loss = criterion(scores, batch.LABEL)\n",
    "            acc = batch_accuracy(scores, batch.LABEL)\n",
    "            \n",
    "            total_acc+=acc.item()\n",
    "            total_loss+=loss.item()\n",
    "    \n",
    "    return total_loss/len(iterator), total_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(train_iterator,\n",
    "                total=len(train_iterator),\n",
    "                leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        out = model(batch.REVIEWS)\n",
    "        scores = out.squeeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(scores, batch.LABEL)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc = batch_accuracy(scores, batch.LABEL)\n",
    "#         loop.set_description(f\"Epoch [])\n",
    "        loop.set_description(f\"Epoch [{epoch}/{NUM_EPOCHS}]\")\n",
    "        loop.set_postfix(loss=loss.item(), train_acc=train_acc.item())\n",
    "                             \n",
    "#         break\n",
    "    \n",
    "    val_loss, val_acc  = evaluate(model, val_iterator, criterion)\n",
    "        \n",
    "    loop.set_postfix(loss=loss.item(), train_acc=train_acc, val_loss=val_loss, val_acc=val_acc)\n",
    " \n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7265335380191534"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4951584507042254"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.722 | Test Acc: 50.00%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('SA_model_1.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, review):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(review)\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokens]\n",
    "#     length = [len(indexed)]\n",
    "    \n",
    "    in_tensor = torch.LongTensor(indexed).to(device)\n",
    "    in_tensor = in_tensor.unsqueeze(1)\n",
    "#     length_tensor = torch.LongTensor(le)\n",
    "\n",
    "    pred = torch.sigmoid(model(in_tensor))\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6613641381263733"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"An awesome movie and no cheesy scences, love it! Must watch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
