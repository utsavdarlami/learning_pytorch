{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build custom Datasets for Text in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources :\n",
    "\n",
    "- https://www.youtube.com/watch?v=9sHcLvVXsns&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agendas \n",
    "- We want to convert text -> numerical values\n",
    "- We need a Vocabulary mapping each word to a index\n",
    "- we need to setup a pytorch dataset to load the data\n",
    "- setup padding of every batch ( all examples should be of same seq_len in a batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "print(\"done importing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images', 'captions.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"./flickr8k\"\n",
    "os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = \"./flickr8k/captions.txt\"\n",
    "caption_df = pd.read_csv(csv)\n",
    "caption_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_vocab = caption_df['caption'].tolist()\n",
    "for_vocab[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A child in a pink dress is climbing up a set of stairs in an entry way ."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_eng.tokenizer(for_vocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two', 'men', 'are', 'ice', 'fishing', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens =  []\n",
    "for word in spacy_eng.tokenizer(for_vocab[99]):\n",
    "    tokens.append(word.text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    \"\"\"\n",
    "        freq_threshold is the threshold for the words to be kept if\n",
    "        their world count frequency is greater than or equal to the freq_threshold\n",
    "    \"\"\"\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.stoi = {\"<PAD>\" : 0,\n",
    "                    \"<SOS>\" : 1,\n",
    "                    \"<EOS>\" : 2,\n",
    "                    \"<UNK>\" : 3}\n",
    "        self.itos = {0:\"<PAD>\",\n",
    "                    1 :\"<SOS>\",\n",
    "                    2 : \"<EOS>\",\n",
    "                    3 : \"<UNK>\"}\n",
    "        \n",
    "    @staticmethod\n",
    "    def tokenizer_english(text):\n",
    "        tokens = []\n",
    "        for word in spacy_eng.tokenizer(text):\n",
    "            tokens.append(word.text.lower())\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        idx = 4\n",
    "        frequencies = {}\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for token in self.tokenizer_english(sentence):\n",
    "                if token not in frequencies:\n",
    "                    frequencies[token] = 0\n",
    "                else :\n",
    "                    frequencies[token]+=1\n",
    "                if frequencies[token] == self.freq_threshold:\n",
    "                    self.stoi[token] = idx\n",
    "                    self.itos[idx] = token\n",
    "                    idx+=1\n",
    "                \n",
    "    def numericalize(self, text):\n",
    "        numeric_text = []\n",
    "        for token in self.tokenizer_english(text):\n",
    "            if token in self.stoi:\n",
    "                numeric_text.append(self.stoi[token])\n",
    "            else:\n",
    "                numeric_text.append(self.stoi[\"<UNK>\"])\n",
    "        return numeric_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, caption_file,transform=None,freq_threshold=5) :\n",
    "        self.root_dir = root_dir\n",
    "        self.caption_df = pd.read_csv(caption_file)\n",
    "        self.transform = transform\n",
    "        self.images = self.caption_df['image']\n",
    "        self.captions = self.caption_df['caption']\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions.tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.caption_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        image_id = self.images[index]\n",
    "        image_path = os.path.join(self.root_dir, image_id)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        token_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        token_caption +=  self.vocab.numericalize(caption)\n",
    "        token_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "\n",
    "        return image, torch.tensor(token_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPaddingCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    def __call__(self,batch):\n",
    "#         images, captions = batch[0],  batch[1]\n",
    "#         all_images = [im.unsqueeze(0) for im in images]\n",
    "#         for item in batch:\n",
    "        images = [item[0].unsqueeze(0) for item in batch]\n",
    "        images = torch.cat(images, dim=0)\n",
    "\n",
    "        captions = [item[1] for item in batch]\n",
    "            \n",
    "#         print(images.shape)\n",
    "#         print(captions.shape)\n",
    "\n",
    "#         all_images = all_iamges\n",
    "#         all_images = torch.cat(images, dim=0)\n",
    "        captions = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        return images, captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pin_memory=True should be set as default as is going to speed up the model by pinning the video memory for the model computations but the internals of how that works I'm as clueless as you. The collate function is for additional processing you want to do on the batch you've collected, so in this case we setup how to load all of these captions but when we actually have the batch we need to make sure they are all padded to be of equal number of time steps, this is done using the collate function\n",
    "1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_folder, caption_file, transform=None, batch_size=32, num_workers=8, shuffle=True, pin_memory=True):\n",
    "    dataset = FlickrDataset(root_folder, caption_file, transform=transform)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(dataset, \n",
    "                        shuffle=shuffle, \n",
    "                        num_workers=num_workers, \n",
    "                        batch_size=batch_size,\n",
    "                        pin_memory=pin_memory,\n",
    "                        collate_fn=AddPaddingCollate(pad_idx=pad_idx)\n",
    "                       )\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    image_folder = \"./flickr8k/images\"\n",
    "    csv = \"./flickr8k/captions.txt\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),transforms.ToTensor()]\n",
    "    )\n",
    "    loader, dataset = get_loader(image_folder, csv, transform=transform)\n",
    "    return loader, dataset\n",
    "# os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([25, 32])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\"  :\n",
    "    loader, dataset = main()\n",
    "    for x, y in loader:\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
